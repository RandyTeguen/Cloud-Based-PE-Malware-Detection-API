import torch
import h5py
import numpy as np
import lief

# Load the MalConv model
def model_fn(model_dir):
    model_path = f"{model_dir}/malconv_model.h5"
    model_state = {}
    with h5py.File(model_path, 'r') as hf:
        for key in hf.keys():
            model_state[key] = torch.tensor(hf[key][:])

    # Initialize the model
    model = MalConv(input_length=2000000, embedding_dim=8, window_size=500, output_dim=1)
    model.load_state_dict(model_state)
    model.eval()
    
    return model

# Function to preprocess the input data
def input_fn(request_body, request_content_type):
    import torch
    from torch.utils.data import DataLoader, TensorDataset
    import numpy as np

    def predict_pe_file(bytez):
        bytez = np.uint8(bytez)
        if len(bytez) < 2000000:
            bytez = np.pad(bytez, (0, 2000000 - len(bytez)), mode='constant')
        features = torch.tensor(bytez[:2000000], dtype=torch.long).unsqueeze(0)
        return features

    if request_content_type == 'application/octet-stream':
        byte_data = bytearray(request_body)
        processed_data = predict_pe_file(byte_data)

        return processed_data
    else:
        raise ValueError(f"Request content type {request_content_type} is not supported by the endpoint.")

# Function that makes predictions
def predict_fn(input_data, model):
    if torch.cuda.is_available():
        device = torch.device("cuda")
    else:
        device = torch.device("cpu")

    model.to(device)
    model.eval()

    with torch.no_grad():
        output = model(input_data)

    return output

# Function to post-process the prediction output
def output_fn(prediction_output, accept_header):
    return prediction_output.numpy().tolist()