{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "history_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HWgfX7xGmF_d"
      },
      "source": [
        "**Revised on 3/5/2024: Changed source files**\n",
        "\n",
        "This is the skeleton code for Task 1 of the midterm project. The files that are downloaded in step 4 are based on the [Ember 2018 dataset](https://arxiv.org/abs/1804.04637), and contain the features (and corresponding labels) extracted from 1 million PE files, split into 80\\% training and 20\\% test datasets. The code used for for feature extraction is available [here](https://colab.research.google.com/drive/16q9bOlCmnTquPtVXVzxUj4ZY1ORp10R2?usp=sharing). However, the preprocessing and featurization process may take up to 3 hours on Google Colab. Hence, I recommend using the processed datasets (Step 4) to speed up your development.\n",
        "\n",
        "Also, note that there is a new optional step 8.5 - To speed up your experiments, you may want to sample the original dataset of 800k training samples and 200k test samples to smaller datasets.\n",
        "\n",
        "**Step 1:** Mount your Google Drive by clicking on \"Mount Drive\" in the Files section (panel to the left of this text.)\n",
        "\n",
        "**Step 2:** Go to Runtime -> Change runtime type and select T4 GPU.\n",
        "\n",
        "**Step 3:** Create a folder in your Google Drive, and rename it to \"vMalConv\"\n",
        "\n",
        "**Step 4:** Download the pre-processed training and test datasets."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "25556b49-77bd-4ac7-d3d9-c3973de170dd",
        "id": "BY72tvX8mF_f"
      },
      "source": [
        "# ~8GB\n",
        "!wget https://dsci6015s24-midterm.s3.amazonaws.com/v2/X_train.dat\n",
        "!wget https://dsci6015s24-midterm.s3.amazonaws.com/v2/X_test.dat\n",
        "!wget https://dsci6015s24-midterm.s3.amazonaws.com/v2/y_train.dat\n",
        "!wget https://dsci6015s24-midterm.s3.amazonaws.com/v2/y_test.dat\n",
        "!wget https://dsci6015s24-midterm.s3.amazonaws.com/v2/metadata.csv"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-03-12 14:22:00--  https://dsci6015s24-midterm.s3.amazonaws.com/v2/X_train.dat\n",
            "Resolving dsci6015s24-midterm.s3.amazonaws.com (dsci6015s24-midterm.s3.amazonaws.com)... 52.217.104.20, 3.5.25.157, 52.217.86.60, ...\n",
            "Connecting to dsci6015s24-midterm.s3.amazonaws.com (dsci6015s24-midterm.s3.amazonaws.com)|52.217.104.20|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 7619200000 (7.1G) [binary/octet-stream]\n",
            "Saving to: ‘X_train.dat’\n",
            "\n",
            "X_train.dat         100%[===================>]   7.10G  45.5MB/s    in 3m 25s  \n",
            "\n",
            "2024-03-12 14:25:25 (35.5 MB/s) - ‘X_train.dat’ saved [7619200000/7619200000]\n",
            "\n",
            "--2024-03-12 14:25:25--  https://dsci6015s24-midterm.s3.amazonaws.com/v2/X_test.dat\n",
            "Resolving dsci6015s24-midterm.s3.amazonaws.com (dsci6015s24-midterm.s3.amazonaws.com)... 52.217.113.89, 3.5.2.158, 54.231.163.89, ...\n",
            "Connecting to dsci6015s24-midterm.s3.amazonaws.com (dsci6015s24-midterm.s3.amazonaws.com)|52.217.113.89|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1904800000 (1.8G) [binary/octet-stream]\n",
            "Saving to: ‘X_test.dat’\n",
            "\n",
            "X_test.dat          100%[===================>]   1.77G  44.7MB/s    in 43s     \n",
            "\n",
            "2024-03-12 14:26:09 (41.9 MB/s) - ‘X_test.dat’ saved [1904800000/1904800000]\n",
            "\n",
            "--2024-03-12 14:26:09--  https://dsci6015s24-midterm.s3.amazonaws.com/v2/y_train.dat\n",
            "Resolving dsci6015s24-midterm.s3.amazonaws.com (dsci6015s24-midterm.s3.amazonaws.com)... 16.182.70.73, 52.217.170.33, 54.231.162.193, ...\n",
            "Connecting to dsci6015s24-midterm.s3.amazonaws.com (dsci6015s24-midterm.s3.amazonaws.com)|16.182.70.73|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 3200000 (3.1M) [binary/octet-stream]\n",
            "Saving to: ‘y_train.dat’\n",
            "\n",
            "y_train.dat         100%[===================>]   3.05M  7.04MB/s    in 0.4s    \n",
            "\n",
            "2024-03-12 14:26:10 (7.04 MB/s) - ‘y_train.dat’ saved [3200000/3200000]\n",
            "\n",
            "--2024-03-12 14:26:10--  https://dsci6015s24-midterm.s3.amazonaws.com/v2/y_test.dat\n",
            "Resolving dsci6015s24-midterm.s3.amazonaws.com (dsci6015s24-midterm.s3.amazonaws.com)... 16.182.70.73, 52.217.170.33, 54.231.162.193, ...\n",
            "Connecting to dsci6015s24-midterm.s3.amazonaws.com (dsci6015s24-midterm.s3.amazonaws.com)|16.182.70.73|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 800000 (781K) [binary/octet-stream]\n",
            "Saving to: ‘y_test.dat’\n",
            "\n",
            "y_test.dat          100%[===================>] 781.25K  2.46MB/s    in 0.3s    \n",
            "\n",
            "2024-03-12 14:26:11 (2.46 MB/s) - ‘y_test.dat’ saved [800000/800000]\n",
            "\n",
            "--2024-03-12 14:26:11--  https://dsci6015s24-midterm.s3.amazonaws.com/v2/metadata.csv\n",
            "Resolving dsci6015s24-midterm.s3.amazonaws.com (dsci6015s24-midterm.s3.amazonaws.com)... 16.182.70.73, 52.217.170.33, 54.231.162.193, ...\n",
            "Connecting to dsci6015s24-midterm.s3.amazonaws.com (dsci6015s24-midterm.s3.amazonaws.com)|16.182.70.73|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 92160330 (88M) [text/csv]\n",
            "Saving to: ‘metadata.csv’\n",
            "\n",
            "metadata.csv        100%[===================>]  87.89M  36.2MB/s    in 2.4s    \n",
            "\n",
            "2024-03-12 14:26:14 (36.2 MB/s) - ‘metadata.csv’ saved [92160330/92160330]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2929a2c7-5bd1-4703-9d28-21c633281153",
        "id": "zjIBXw0TmF_i"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1FvyLuyZmF_j"
      },
      "source": [
        "**Step 5:** Copy the downloaded files to vMalConv"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ALdkI32dmF_k"
      },
      "source": [
        "!cp /content/X_train.dat /content/drive/MyDrive/vMalConv/X_train.dat\n",
        "!cp /content/X_test.dat /content/drive/MyDrive/vMalConv/X_test.dat\n",
        "!cp /content/y_train.dat /content/drive/MyDrive/vMalConv/y_train.dat\n",
        "!cp /content/y_test.dat /content/drive/MyDrive/vMalConv/y_test.dat\n",
        "!cp /content/metadata.csv /content/drive/MyDrive/vMalConv/metadata.csv"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wPMXLTMImF_l"
      },
      "source": [
        "**Step 6:** Download and install Ember:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "32b75254-a9f8-4c69-83ce-a6c2665da90d",
        "id": "oNeZterUmF_m"
      },
      "source": [
        "!pip install git+https://github.com/PFGimenez/ember.git"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting git+https://github.com/PFGimenez/ember.git\n",
            "  Cloning https://github.com/PFGimenez/ember.git to /tmp/pip-req-build-5pux_oc5\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/PFGimenez/ember.git /tmp/pip-req-build-5pux_oc5\n",
            "  Resolved https://github.com/PFGimenez/ember.git to commit 3b82fe63069884882e743af725d29cc2a67859f1\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install lief"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "484c6de7-2b57-429b-ccb7-c6feef291e59",
        "id": "fhAizi-ymF_o"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: lief in /usr/local/lib/python3.10/dist-packages (0.14.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9GD5spwmmF_p"
      },
      "source": [
        "**Step 7:** Read vectorized features from the data files."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c603f4c0-ba2d-4d83-e8d5-6579a98e345a",
        "id": "lM3gZpSKmF_q"
      },
      "source": [
        "import ember\n",
        "X_train, y_train, X_test, y_test = ember.read_vectorized_features(\"drive/MyDrive/vMalConv/\")\n",
        "metadata_dataframe = ember.read_metadata(\"drive/MyDrive/vMalConv/\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING: EMBER feature version 2 were computed using lief version 0.9.0-\n",
            "WARNING:   lief version 0.14.1-bae887e0 found instead. There may be slight inconsistencies\n",
            "WARNING:   in the feature calculations.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AAT3fDIlmF_r"
      },
      "source": [
        "**Step 8:** Get rid of rows with no labels."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1mvzbedqmF_s"
      },
      "source": [
        "labelrows = (y_train != -1)\n",
        "X_train = X_train[labelrows]\n",
        "y_train = y_train[labelrows]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9KQjuqF0mF_s"
      },
      "source": [
        "import h5py\n",
        "h5f = h5py.File('X_train.h5', 'w')\n",
        "h5f.create_dataset('X_train', data=X_train)\n",
        "h5f.close()\n",
        "h5f = h5py.File('y_train.h5', 'w')\n",
        "h5f.create_dataset('y_train', data=y_train)\n",
        "h5f.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1h0raFBxmF_t"
      },
      "source": [
        "!cp /content/X_train.h5 /content/drive/MyDrive/vMalConv/X_train.h5\n",
        "!cp /content/y_train.h5 /content/drive/MyDrive/vMalConv/y_train.h5"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Optional Step 8.5:** To speed up your experiments, you may want to sample the original dataset of 800k training samples and 200k test samples to smaller datasets. You can use the [Pandas Dataframe sample() method](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.sample.html), or come up with your own sampling methodology. Be mindful of the fact that the database is heavily imbalanced."
      ],
      "metadata": {
        "id": "ZbZQE1WumF_v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import h5py\n",
        "\n",
        "# Converting numpy arrays to Pandas DataFrames\n",
        "X_train_df = pd.DataFrame(X_train)\n",
        "y_train_df = pd.DataFrame(y_train)\n",
        "X_test_df = pd.DataFrame(X_test)\n",
        "y_test_df = pd.DataFrame(y_test)\n",
        "\n",
        "# Sampling only a portion of the original dataset to create a smaller dataset\n",
        "sample_size = 50000\n",
        "\n",
        "# Sampling the training dataset\n",
        "sampled_X_train = X_train_df.sample(n=sample_size, random_state=1)\n",
        "sampled_y_train = y_train_df.loc[X_train_df.index.isin(sampled_X_train.index)]\n",
        "\n",
        "# Sampling the test dataset\n",
        "sampled_X_test = X_test_df.sample(n=sample_size, random_state=1)\n",
        "sampled_y_test = y_test_df.loc[X_test_df.index.isin(sampled_X_test.index)]\n",
        "\n",
        "# Saving the sampled datasets to h5 files\n",
        "h5f = h5py.File('sampled_X_train.h5', 'w')\n",
        "h5f.create_dataset('sampled_X_train', data=sampled_X_train)\n",
        "h5f.close()\n",
        "\n",
        "h5f = h5py.File('sampled_y_train.h5', 'w')\n",
        "h5f.create_dataset('sampled_y_train', data=sampled_y_train)\n",
        "h5f.close()\n",
        "\n",
        "h5f = h5py.File('sampled_X_test.h5', 'w')\n",
        "h5f.create_dataset('sampled_X_test', data=sampled_X_test)\n",
        "h5f.close()\n",
        "\n",
        "h5f = h5py.File('sampled_y_test.h5', 'w')\n",
        "h5f.create_dataset('sampled_y_test', data=sampled_y_test)\n",
        "h5f.close()"
      ],
      "metadata": {
        "id": "GP6z5xscmF_w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Copying the sampled datasets to Google Drive\n",
        "!cp /content/sampled_X_train.h5 /content/drive/MyDrive/vMalConv/sampled_X_train.h5\n",
        "!cp /content/sampled_y_train.h5 /content/drive/MyDrive/vMalConv/sampled_y_train.h5\n",
        "!cp /content/sampled_X_test.h5 /content/drive/MyDrive/vMalConv/sampled_X_test.h5\n",
        "!cp /content/sampled_y_test.h5 /content/drive/MyDrive/vMalConv/sampled_y_test.h5"
      ],
      "metadata": {
        "id": "ANRpS9tHmF_x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HKUpKTfXmF_y"
      },
      "source": [
        "> **Task 1:** Complete the following code to build the architecture of MalConv in PyTorch:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "362f277a-a1a0-4ea8-a290-3a0a0181d736",
        "id": "e83yHe77mF_2"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import h5py\n",
        "\n",
        "# Reading the sampled training data from Colab environment\n",
        "h5f_X = h5py.File('/content/drive/MyDrive/vMalConv/sampled_X_train.h5', 'r')\n",
        "sampled_X_train_tensor = torch.tensor(h5f_X['sampled_X_train'][:], dtype=torch.long)\n",
        "h5f_X.close()\n",
        "\n",
        "h5f_y = h5py.File('/content/drive/MyDrive/vMalConv/sampled_y_train.h5', 'r')\n",
        "sampled_y_train_tensor = torch.tensor(h5f_y['sampled_y_train'][:], dtype=torch.long)\n",
        "h5f_y.close()\n",
        "\n",
        "# Create and initialize the MalConv model\n",
        "class MalConv(nn.Module):\n",
        "    def __init__(self, input_length=2000000, embedding_dim=8, window_size=500, output_dim=1):\n",
        "        super(MalConv, self).__init__()\n",
        "        self.embed = nn.Embedding(256, embedding_dim, padding_idx=0)  # 256 unique bytes, embedding dimension\n",
        "        self.conv1 = nn.Conv1d(in_channels=embedding_dim, out_channels=128, kernel_size=32, stride=32)\n",
        "        self.conv2 = nn.Conv1d(in_channels=128, out_channels=128, kernel_size=32, stride=32)\n",
        "        self.fc1 = nn.Linear(128, 128)\n",
        "        self.fc2 = nn.Linear(128, output_dim)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.embed(x.clamp(min=0, max=255))  # Ensure indices are within the valid range\n",
        "        x = x.transpose(1, 2)  # Conv1d expects (batch_size, channels, length)\n",
        "        x = self.conv1(x)\n",
        "        x = torch.relu(x)\n",
        "        x = self.conv2(x)\n",
        "        x = torch.relu(x)\n",
        "        x = torch.squeeze(torch.max(x, dim=2)[0])  # Global max pooling\n",
        "        x = self.fc1(x)\n",
        "        x = torch.relu(x)\n",
        "        x = self.fc2(x)\n",
        "        x = self.sigmoid(x)\n",
        "        return x\n",
        "\n",
        "# Example input using the sampled training data\n",
        "input_length = 2000000  # The fixed length for each input file\n",
        "batch_size = 4\n",
        "example_input = sampled_X_train_tensor[:batch_size, :input_length]  # Using the first 4 examples from sampled_X_train\n",
        "\n",
        "# Create and initialize the MalConv model\n",
        "model = MalConv(input_length=input_length, embedding_dim=8, window_size=500, output_dim=1)\n",
        "\n",
        "# Use the MalConv model to make predictions\n",
        "output = model(example_input)\n",
        "print(output)  # The output probabilities for each example\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0.5077],\n",
            "        [0.5061],\n",
            "        [0.5063],\n",
            "        [0.5057]], grad_fn=<SigmoidBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vJcrWQ9WmF_4"
      },
      "source": [
        "**Step 8:** Partial fit the standardScaler to avoid overloading the memory:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H4q5OfK9v9iN"
      },
      "source": [
        "import h5py\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Load sampled_X_train\n",
        "drive_path = '/content/sampled_X_train.h5'\n",
        "with h5py.File(drive_path, 'r') as hf:\n",
        "    sampled_X_train = hf['sampled_X_train'][:]\n",
        "\n",
        "\n",
        "mms = StandardScaler()\n",
        "for x in range(0, len(sampled_X_train), 100000):\n",
        "    mms.partial_fit(sampled_X_train[x:x+100000])\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B33Oa1sTxdB0"
      },
      "source": [
        "sampled_X_train = mms.transform(sampled_X_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V_vl5yrex0yY"
      },
      "source": [
        "# Read sampled_y_train\n",
        "drive_path_sampled_y_train = '/content/sampled_y_train.h5'\n",
        "with h5py.File(drive_path_sampled_y_train, 'r') as hf:\n",
        "    sampled_y_train = hf['sampled_y_train'][:]\n",
        "\n",
        "# Reshape X_train_sampled and y_train_sampled\n",
        "import numpy as np\n",
        "sampled_X_train = np.reshape(sampled_X_train, (-1, 1, 2381))\n",
        "sampled_y_train = np.reshape(sampled_y_train, (-1, 1, 1))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Load, Tensorize, and Split** The following code takes care of converting the training data into Torch Tensors, and then splits it into 80% training and 20% validation datasets."
      ],
      "metadata": {
        "id": "b1iRXFtuvCps"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import h5py\n",
        "import numpy as np\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Assuming MalConv class definition is already provided as above\n",
        "\n",
        "# Read sampled_y_train\n",
        "drive_path_sampled_y_train = '/content/sampled_y_train.h5'\n",
        "with h5py.File(drive_path_sampled_y_train, 'r') as hf:\n",
        "    sampled_y_train = hf['sampled_y_train'][:]\n",
        "\n",
        "# Load sampled_X_train\n",
        "drive_path = '/content/sampled_X_train.h5'\n",
        "with h5py.File(drive_path, 'r') as hf:\n",
        "    sampled_X_train = hf['sampled_X_train'][:]\n",
        "\n",
        "\n",
        "# Convert your numpy arrays to PyTorch tensors\n",
        "sampled_X_train_tensor = torch.tensor(sampled_X_train, dtype=torch.long)\n",
        "sampled_y_train_tensor = torch.tensor(sampled_y_train, dtype=torch.float32)\n",
        "\n",
        "# Split the data into training and validation sets (80% training, 20% validation)\n",
        "sampled_X_train_split, sampled_X_val_split, sampled_y_train_split, sampled_y_val_split = train_test_split(\n",
        "    sampled_X_train_tensor, sampled_y_train_tensor, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Create TensorDatasets and DataLoaders for training and validation sets\n",
        "sampled_train_dataset = TensorDataset(sampled_X_train_split, sampled_y_train_split)\n",
        "sampled_val_dataset = TensorDataset(sampled_X_val_split, sampled_y_val_split)\n",
        "\n",
        "batch_size = 16\n",
        "sampled_train_loader = DataLoader(sampled_train_dataset, batch_size=batch_size, shuffle=True)\n",
        "sampled_val_loader = DataLoader(sampled_val_dataset, batch_size=batch_size, shuffle=False)\n"
      ],
      "metadata": {
        "id": "EpA82c0CODwS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2zMgth6McCqV"
      },
      "source": [
        "> **Task 2:** Complete the following code to train the model on the GPU for 15 epochs, with a batch size of 64. If you are on Google Colab, don't forget to change the kernel in Runtime -> Change runtime type -> T4 GPU."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "import os\n",
        "\n",
        "# Assuming MalConv class definition is already provided as above\n",
        "\n",
        "# Initialize the MalConv model\n",
        "model = MalConv()\n",
        "\n",
        "# Move model to GPU if available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "# Loss function and optimizer\n",
        "criterion = nn.BCELoss()  # Binary Cross-Entropy Loss for binary classification\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)  # Adjust learning rate as needed\n",
        "\n",
        "# Directory to save model checkpoints\n",
        "save_dir = \"/content\"\n",
        "\n",
        "# Training Loop with Validation\n",
        "num_epochs = 15  # Adjust the number of epochs as needed\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()  # Set model to training mode\n",
        "    running_loss = 0.0\n",
        "\n",
        "    for inputs, labels in sampled_train_loader:\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "        # Zero the parameter gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs.squeeze(), labels.squeeze())\n",
        "\n",
        "        # Backward pass and optimize\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "\n",
        "    print(f'Epoch {epoch+1}, Training Loss: {running_loss/len(sampled_train_loader)}')\n",
        "\n",
        "    # Validation step\n",
        "    model.eval()  # Set model to evaluation mode\n",
        "    val_loss = 0.0\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in sampled_val_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs.squeeze(), labels.squeeze())\n",
        "            val_loss += loss.item()\n",
        "    print(f'Validation Loss: {val_loss/len(sampled_val_loader)}')\n",
        "\n",
        "    # Save checkpoint every 5 epochs\n",
        "    if (epoch + 1) % 5 == 0:\n",
        "        checkpoint_path = os.path.join(save_dir, f'model_epoch_{epoch+1}.pt')\n",
        "        torch.save(model.state_dict(), checkpoint_path)\n",
        "        print(f'Model checkpoint saved to {checkpoint_path}')\n"
      ],
      "metadata": {
        "id": "iv7piF7dp0lm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0e844850-0e87-4759-ff42-abc96767af1a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Training Loss: 0.693395237660408\n",
            "Validation Loss: 0.6931996603965759\n",
            "Epoch 2, Training Loss: 0.6932449769496918\n",
            "Validation Loss: 0.6931774196624756\n",
            "Epoch 3, Training Loss: 0.6932324339866638\n",
            "Validation Loss: 0.6931317331314087\n",
            "Epoch 4, Training Loss: 0.6931587614774704\n",
            "Validation Loss: 0.6931059553146363\n",
            "Epoch 5, Training Loss: 0.6931937012434006\n",
            "Validation Loss: 0.6931151913642883\n",
            "Model checkpoint saved to /content/model_epoch_5.pt\n",
            "Epoch 6, Training Loss: 0.6931794244289399\n",
            "Validation Loss: 0.6931455094337463\n",
            "Epoch 7, Training Loss: 0.6931633016586304\n",
            "Validation Loss: 0.6931056371688843\n",
            "Epoch 8, Training Loss: 0.6931797386884689\n",
            "Validation Loss: 0.6930944939613343\n",
            "Epoch 9, Training Loss: 0.693186948299408\n",
            "Validation Loss: 0.6931421483039856\n",
            "Epoch 10, Training Loss: 0.6931722944498062\n",
            "Validation Loss: 0.6931598110198974\n",
            "Model checkpoint saved to /content/model_epoch_10.pt\n",
            "Epoch 11, Training Loss: 0.6931765324354172\n",
            "Validation Loss: 0.693188800907135\n",
            "Epoch 12, Training Loss: 0.6931635766267776\n",
            "Validation Loss: 0.6930928957939148\n",
            "Epoch 13, Training Loss: 0.6931752282619477\n",
            "Validation Loss: 0.6931027994155884\n",
            "Epoch 14, Training Loss: 0.6931626671552658\n",
            "Validation Loss: 0.6931480396270752\n",
            "Epoch 15, Training Loss: 0.6931567072868348\n",
            "Validation Loss: 0.6932090881347657\n",
            "Model checkpoint saved to /content/model_epoch_15.pt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Task 3:** Complete the following code to evaluate your trained model on the test data."
      ],
      "metadata": {
        "id": "obToo1WZtD4m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
        "\n",
        "# Set up Stratified K-Fold cross-validation\n",
        "k_folds = 5\n",
        "skf = StratifiedKFold(n_splits=k_folds, shuffle=True, random_state=42)\n",
        "\n",
        "# Lists to store evaluation metrics for each fold\n",
        "accuracies = []\n",
        "precisions = []\n",
        "recalls = []\n",
        "\n",
        "for fold, (train_idx, val_idx) in enumerate(skf.split(sampled_X_train_tensor, sampled_y_train_tensor)):\n",
        "    # Split data into training and validation sets for this fold\n",
        "    X_train_fold, X_val_fold = sampled_X_train_tensor[train_idx], sampled_X_train_tensor[val_idx]\n",
        "    y_train_fold, y_val_fold = sampled_y_train_tensor[train_idx], sampled_y_train_tensor[val_idx]\n",
        "\n",
        "    # Create TensorDatasets and DataLoaders for training and validation sets\n",
        "    train_dataset = TensorDataset(X_train_fold, y_train_fold)\n",
        "    val_dataset = TensorDataset(X_val_fold, y_val_fold)\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "    # Initialize the MalConv model\n",
        "    model = MalConv(input_length=input_length, embedding_dim=8, window_size=500, output_dim=1)\n",
        "    model.to(device)\n",
        "\n",
        "    # Evaluate the model on the validation set\n",
        "    model.eval()\n",
        "    predictions = []\n",
        "    ground_truth = []\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in val_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            outputs = model(inputs)\n",
        "            predicted = torch.round(outputs)\n",
        "            predictions.extend(predicted.cpu().numpy())\n",
        "            ground_truth.extend(labels.cpu().numpy())\n",
        "\n",
        "    # Compute evaluation metrics for this fold\n",
        "    accuracy = accuracy_score(ground_truth, predictions)\n",
        "    precision = precision_score(ground_truth, predictions, zero_division=0)\n",
        "    recall = recall_score(ground_truth, predictions, zero_division=0)\n",
        "\n",
        "    accuracies.append(accuracy)\n",
        "    precisions.append(precision)\n",
        "    recalls.append(recall)\n",
        "\n",
        "    print(f\"Fold {fold + 1}: Accuracy={accuracy:.4f}, Precision={precision:.4f}, Recall={recall:.4f}\")\n",
        "\n",
        "# Calculate the average metrics across all folds\n",
        "avg_accuracy = sum(accuracies) / k_folds\n",
        "avg_precision = sum(precisions) / k_folds\n",
        "avg_recall = sum(recalls) / k_folds\n",
        "\n",
        "print(f\"Average Accuracy: {avg_accuracy:.4f}\")\n",
        "print(f\"Average Precision: {avg_precision:.4f}\")\n",
        "print(f\"Average Recall: {avg_recall:.4f}\")\n"
      ],
      "metadata": {
        "id": "83wqvS9jqppe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "175dd981-5a95-44a0-ef1b-f85cf1aad9df"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 1: Accuracy=0.5017, Precision=0.5017, Recall=1.0000\n",
            "Fold 2: Accuracy=0.5017, Precision=0.5017, Recall=1.0000\n",
            "Fold 3: Accuracy=0.5017, Precision=0.5017, Recall=1.0000\n",
            "Fold 4: Accuracy=0.4980, Precision=0.4979, Recall=0.0708\n",
            "Fold 5: Accuracy=0.4982, Precision=0.0000, Recall=0.0000\n",
            "Average Accuracy: 0.5003\n",
            "Average Precision: 0.4006\n",
            "Average Recall: 0.6142\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Task 4:** Comment on the results in this text box.\n",
        "\n",
        "I sampled the original dataset of 800000 training samples and 200000 test samples to 50000 using the pandas dataframe sample() method in step 8.5 resulting to a heavily imbalanced dataset.\n",
        "Based on the output in Task 3, it seems that the model's performance on the test data is close to random guessing, as the accuracy is around 50%.\n",
        "\n",
        "The precision score is 0.4006. This indicates my model's ability to correctly classify positive samples is relatively low. A low precision indicates that my model may have a high false positive rate.\n",
        "\n",
        "As for the recall score of 0.6142, it suggests that my model's ability to correctly identify positive samples is moderate. This indicates that my model may have a high false negative rate."
      ],
      "metadata": {
        "id": "W6fLYYxps91N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Storing my model\n",
        "\n",
        "import torch\n",
        "import h5py\n",
        "import os\n",
        "\n",
        "# Assuming 'model' is your trained MalConv model\n",
        "# Directory to save the model checkpoints\n",
        "save_dir = \"/content\"\n",
        "\n",
        "# Save the entire model\n",
        "model_path = os.path.join(save_dir, 'malconv_model.h5')\n",
        "model_state = model.state_dict()\n",
        "\n",
        "with h5py.File(model_path, 'w') as hf:\n",
        "    for key in model_state.keys():\n",
        "        hf.create_dataset(key, data=model_state[key].numpy())\n",
        "\n",
        "print(f\"Model saved to {model_path}\")\n"
      ],
      "metadata": {
        "id": "wfMhFww0AhbQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e68dc4bb-0739-4ff4-f44e-f588ee0ec355"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model saved to /content/malconv_model.h5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Extra Task**: Function that takes a PE File as its argument, runs it through the trained model, and returns the output: Malware or Benign.\n"
      ],
      "metadata": {
        "id": "7OckVJdrtEZD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import h5py\n",
        "import lief\n",
        "import numpy as np\n",
        "\n",
        "# Load the saved model\n",
        "model_path = \"/content/malconv_model.h5\"\n",
        "model_state = {}\n",
        "with h5py.File(model_path, 'r') as hf:\n",
        "    for key in hf.keys():\n",
        "        model_state[key] = torch.tensor(hf[key][:])\n",
        "\n",
        "# Initialize the model\n",
        "model = MalConv(input_length=2000000, embedding_dim=8, window_size=500, output_dim=1)\n",
        "model.load_state_dict(model_state)\n",
        "model.eval()\n",
        "\n",
        "def predict_pe_file(file_path):\n",
        "    # Load the PE file\n",
        "    try:\n",
        "        pe = lief.parse(file_path)\n",
        "    except lief.read_error:\n",
        "        return \"Invalid PE file\"\n",
        "\n",
        "    # Extract features from the PE file\n",
        "    bytez = np.fromfile(file_path, dtype=np.uint8)\n",
        "    if len(bytez) < 2000000:\n",
        "        bytez = np.pad(bytez, (0, 2000000 - len(bytez)), mode='constant')\n",
        "    features = torch.tensor(bytez[:2000000], dtype=torch.long).unsqueeze(0)\n",
        "\n",
        "    # Run the model on the features\n",
        "    with torch.no_grad():\n",
        "        output = model(features)\n",
        "\n",
        "    # Return the prediction\n",
        "    return \"Malware\" if output.item() > 0.5 else \"Benign\"\n",
        "\n",
        "# Example usage\n",
        "file_path = \"/content/putty.exe\"\n",
        "prediction = predict_pe_file(file_path)\n",
        "print(f\"The file {file_path} is predicted as {prediction}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J5fV6ertOQAI",
        "outputId": "fa909368-11f0-4cbe-e0d1-844cdaa8ef12"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The file /content/putty.exe is predicted as Benign\n"
          ]
        }
      ]
    }
  ]
}